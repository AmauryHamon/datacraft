[
    {
      "Title": "Red the ocean around U ",
      "Date": "2024 - 2025",
      "Authors": "Crosslucid",
      "Concept": "Simulation",
      "Category": "",
      "Synopsis": "Red The Ocean Around U is a commission by LAS Art Foundation, in symbiotic collaboration with the sci-fi writer Orion Facey. \nAccessible on LAS website until December 2025, this distinctive online experience invites participants into an immersive and continuously evolving narrative ecosystem, highlighting the notion of games as a poetic manipulation of agency.\n",
      "Why do I care?": "",
      "Cf.": "",
      "Type of dataset (input)": "Visitor choices ",
      "Result (output)": "Generative narrative",
      "Formula": "At the heart of this intra-active experience lies a generative multi-agential AI ecosystem that dynamically transforms in real-time, shaped by user interactions, inputs, and various short-term and long-term memories. ",
      "Tools": "LLM / a generative multi-agential AI ecosystem ",
      "Context": "LAS Art Foundation",
      "Additional info": "Each visitor's journey intricately molds the environment, leaving a unique imprint influenced by their choices, duration of engagement, browser traces, and the collective behavior of all previous explorers. \n\nEvery choice, answer and path meshes into an emergent, fluid visual narrative output, as unpredictable and rich as the red ocean tides.\n\nGuiding users through this otherworldly landscape is Claude, a character driven by a Large Language Model (LLM). Each interaction with Claude influences the storyline, ensuring a distinct pathway for every participant Red the Ocean Around U reverses traditional interactions with AI: users engage with Claude's enigmatic questions generated by LLMs trained on Facey's writings and the collaborators’ emergent questions. The voice guiding this experience is a sophisticated composite that seamlessly merges the voices of the artists and the author.\n\nThe experience is part of a series of web-based commissions at LAS, To Fracture Time is Create an Otherwise curated by Barbara Cueto that brings together artists and writers who use science fiction as both a medium and inspiration to create alternative realities across media. They work together to create imaginaries that re-understand the future as a plural multiverse of world-building possibilities.",
      "Links": "https://crosslucid.zone/red-the-ocean-around-u-1",
      "Image": "",
      "Source image": "Red The Ocean Around U (2024), Crosslucid",
      "Addtional links": "https://red-the-ocean.las-art.foundation/"
    },
    {
      "Title": "Slime machine",
      "Date": "2024",
      "Authors": "Mathilde Sartori",
      "Concept": "Simulation",
      "Category": "",
      "Synopsis": "This research explores the concept of intelligence, the view of machines and their relationship in a more than human way.\nThe focus of my research is slime mould, and in particular the Physarum Polycephalum species, a non-binary, decentralized, random organism used as a model for thinking about and then developing a new form of computing.\nI begin by rethinking intelligence, challenging the brain-centric view that has dominated our society and trying to understand it in all its forms, trying to remove the anthropocentric lens and analyze it in its more-than-human forms. Then, I examine how machines can be reimagined, designing new relationships and better ways to coexist with non-human entities, following an ecology of technology.\nThe research includes the development of a computer vision algorithm inspired by the behavior of slime moulds, with a focus on edge detection. This algorithm, aaardm.py, starts from randomness and searches for solutions rather than predicting them, providing a philosophical and practical framework for a more ecological and inclusive form of computing.",
      "Why do I care?": "",
      "Cf.": "",
      "Type of dataset (input)": "the Physarum Polycephalum species, a non-binary, decentralized, random organism ",
      "Result (output)": "a new form of computing",
      "Formula": "development of a computer vision algorithm inspired by the behavior of slime moulds, with a focus on edge detection",
      "Tools": "",
      "Context": "Diploma project",
      "Additional info": "Through this research I wanted to demonstrate that there are other ways of doing technology, that integrate with the more-than-human world in an harmoniously way, keeping nature at the center of the process and that the conception of a new form of computing is not only material in nature, but refers back to a new way of thinking.\n\n",
      "Links": "https://matildee3.github.io/matilde/project/slime.html",
      "Image": "",
      "Source image": "Slime machine, Mathilde Sartori, 2024",
      "Addtional links": "https://archive.org/details/slime_machines/page/n9/mode/2up\n\nhttps://github.com/matildee3/slime-machine\nhttps://matildee3.github.io/matilde/file/final.mp4"
    },
    {
      "Title": "I KNEW THAT IF I WALKED IN YOUR FOOTSTEPS, IT WOULD BECOME A RITUAL",
      "Date": "2021",
      "Authors": "AARATI AKKAPEDDI",
      "Concept": "GAN",
      "Category": "",
      "Synopsis": "a machine learning model based on the artist family photographs that is able to create its own images that combine and emulate characteristics of their family photos.",
      "Why do I care?": "",
      "Cf.": "",
      "Type of dataset (input)": "Personal family photo",
      "Result (output)": "Images, Video, Cyanolumen Prints",
      "Formula": "trained GAN on personal images",
      "Tools": "GAN and First Order Motion Model for the video part ",
      "Context": "Center for Book Arts",
      "Additional info": "The form of this work is a video piece and series of prints. For the video, family members were asked to speak about specific photographs. Images generated from the machine learning model are animated using footage from these interviews. The facial expressions mimic the original expressions of the family member and the audio is from their original voice. The photographs that are the subject of the interviews are also shown alongside the speaker. Surrounding the video, are prints of the generated images mixed with actual family photographs.",
      "Links": "https://aarati.online/ritual.html",
      "Image": "",
      "Source image": "I KNEW THAT IF I WALKED IN YOUR FOOTSTEPS, IT WOULD BECOME A RITUAL, AARATI AKKAPEDDI, 2021, ",
      "Addtional links": "https://vimeo.com/721567723\nhttps://docs.google.com/document/d/13auFC4LawO6rWaiUpcCemWiSTE7hZNCEUB_G9-dRprQ/edit?tab=t.0\n"
    },
    {
      "Title": "Superprompt",
      "Date": "2023",
      "Authors": "Sarah Rothberg",
      "Concept": "Doppelgänger",
      "Category": "",
      "Synopsis": "is a body of work made in 2023 about conversation in the evolving landscape of spatial computing and artificial intelligence. SUPERPROMPT takes form as custom software (built using unity, web technologies, large language models, and VR) used to produce livestreamed avatar performances, interactive installation, and video.",
      "Why do I care?": "",
      "Cf.": "",
      "Type of dataset (input)": "Motion capture, large language models",
      "Result (output)": "Generated avatars",
      "Formula": "",
      "Tools": "unity, web technologies, large language models, and VR",
      "Context": "Various contexts and performances",
      "Additional info": "Central to SUPERPROMPT is an avatar that is sometimes inhabited by me, sometimes by anonymous guests (as an extension of the work NEW MEETINGS), and sometimes by a conversational large language model (which I named “Sophie” after a conversation in which ChatGPT had hallucinated a performance I had done for the Whitney 2019 biennial by the same name). ",
      "Links": "https://sarahrothberg.com/SUPERPROMPT",
      "Image": "",
      "Source image": "Superprompt, Sarah Rothberg, 2023",
      "Addtional links": "https://www.youtube.com/watch?v=bkzbvmjOMUQ\nhttps://sarahrothberg.com/NEW-MEETINGS"
    },
    {
      "Title": "Once upon a garden",
      "Date": "2021 - 2024",
      "Authors": "Linda Dounia Rebeiz",
      "Concept": "Hallucination",
      "Category": "",
      "Synopsis": "Once Upon A Garden is a speculative archive of critically endangered and extinct flora that we have little to no records of, and therefore no way to remember. In this project, I use Artificial Intelligence as a time machine to go back in time and piece together data about flowers we didn’t bother recording using data that we did record. ",
      "Why do I care?": "",
      "Cf.": "",
      "Type of dataset (input)": "records of extinct and critically endangered flora from the Sahel region of West Africa, as classified by the International Union for Conservation of Nature. This means photographic records from the internet (seldom available), photographic records from physical archives (national archives, family photo books, journals, etc.), herbarium records, encyclopaedic records, and academic records. From these records, individual species are identified and tagged with all available information about it (texts and images).",
      "Result (output)": "50 emergent species of speculative flora in West Africa across 5 chapters ",
      "Formula": "Chapter 1:  trained a GAN on real and imagined visual records of extinct and critically endangered flora from the Sahel region of West Africa.\nChapter 2: text-to-image diffusion. Using the text data gathered at the onset of the project as prompt, I endeavoured to bring more life into the GAN outputs.\nChapter 3: compare different diffusion models using a similar approach to Chapter II, starting with text data from the initial database for the project, and using the GAN to Text-to-Image pipeline outputs as reference images. \nChapter 4: Starting in the fourth chapter though, I start the process of rewilding this body of work and increasing entropy. First, lossiness is added to flowers from the fourth chapter by adding non-organic materials to their make-up. Second, gardens from the first chapter are re-explored through wilder and more chaotic floral arrangements. \n\nThrough an Image-to-Image pipeline, flowers are corrupted",
      "Tools": "",
      "Context": "Various exhibitions and commisions ",
      "Additional info": "To be able to fill gaps in the world’s collective memory with synthetic memory is a unique opportunity that AI offers today. Between 2021 and now, Once Upon A Garden has speculated on what the flora population in West Africa (where I am from and lives) might have looked like decades ago using increasingly faster and more refined models. \n\nThe resulting body of work, while increasingly more photo-realistic as AI became more powerful over the years, cannot be mistaken for reality due to the thin and fragmented data available for training. Global efforts to record disappearing biodiversity have not been consistent across time and geographies. In some places, when interrogating what’s been lost, we have no photographic records and can only rely on herbarium records dating back to colonial expeditions. \n\nOnce Upon A Garden ultimately makes a case for more even efforts in recording biodiversity loss across the globe. It also shows an alternative use of AI that demonstrates how this technology can hold a mirror to what we care to remember and therefore record. Incidentally, this mirror also shows us what gets left behind in the making of an increasingly more influential tool in our lives and societies.",
      "Links": "https://lindarebeiz.com/once-upon-a-garden",
      "Image": "",
      "Source image": "Synthetic Rot, 2024, Linda Dounia Rebeiz ",
      "Addtional links": "\nhttps://vimeo.com/1014516540\nhttps://vimeo.com/1019386571"
    },
    {
      "Title": "Coalesce",
      "Date": "2023",
      "Authors": "Luis Rodriguez",
      "Concept": "Hallucination",
      "Category": "",
      "Synopsis": "Coalesce is a speculative design research project which explores the use of artificial intelligence to create a gender-fluid design language. By using AI image blending and text-to-image AI models, it is possible to create intersections between unnecessarily gendered products and question the biases and stereotypes that are linked to gender identities within the product design industry.\n\n",
      "Why do I care?": "",
      "Cf.": "",
      "Type of dataset (input)": "razors for women and men and physical prototypes made through 3D sculpting and modelling as reinterpretations of the AI-designed products",
      "Result (output)": "gender fluid razors ",
      "Formula": "AI image blending and text-to-image AI models",
      "Tools": "",
      "Context": "Diploma project",
      "Additional info": "The project consists of an interactive installation that showcases the results of blending razors for women and men through a trained AI model and physical prototypes made through 3D sculpting and modelling as reinterpretations of the AI-designed products.",
      "Links": "https://ecal.ch/en/feed/projects/7601/coalesce/",
      "Image": "",
      "Source image": "Coalesce, Luis Rodriguez, 2023",
      "Addtional links": ""
    },
    {
      "Title": "The shell record ",
      "Date": "2021",
      "Authors": "Anna Ridler ",
      "Concept": "Collection",
      "Category": "",
      "Synopsis": "The Shell Record is both a dataset of shells gathered by that artist from the foreshore of the river Thames in early 2021 and moving image pieces generated by using a GAN trained on this imagery. It explores ideas around collecting, value and trade and links to the history of shells being used as stores of value as one of the earliest forms of currency. It is also a work that is a record of the shells of the Thames, forever written into the blockchain, at this particular moment. A recent scientific paper shows that shells that have been in the river since the end of the last Ice Age have now become rare, replaced by other non-native species and that this will eventually become a time-marker for the Anthropocene as they become fossils in the strata.\n",
      "Why do I care?": "",
      "Cf.": "",
      "Type of dataset (input)": "Images of shells gathered from the foreshore of the river",
      "Result (output)": "A moving image / NFT",
      "Formula": "Personal dataset ",
      "Tools": "",
      "Context": "Sotheby's Digitally Native sale, 2021",
      "Additional info": "‘The Shell Record’ is a project that is made up of an image of the hundreds of shells that I collected from the foreshore of the Thames organised into a grid and used as a training set, and a moving image piece that resulted from training a model on them. The piece was created with the intention of being sold as an NFT as part of the Sotheby’s Digitally Native sale in 2021 and ideas around ownership, trade and value are threaded through it, conceptually and practically.",
      "Links": "https://annaridler.com/the-shell-record-2021",
      "Image": "",
      "Source image": "The Shell record, Anna Ridler, 2021",
      "Addtional links": "https://vimeo.com/729584846\nhttps://www.youtube.com/watch?v=Fz1F24h1DP4"
    },
    {
      "Title": "The inquisitive mind",
      "Date": "2024",
      "Authors": "oio Studio",
      "Concept": "Collection",
      "Category": "",
      "Synopsis": "Inquis­it­ive minds listen, look, scan, and ask ques­tions. They feed off all those little elements that we do not think of archiv­ing – that they glean from the inter­net, from docu­ments sent by design­ers or from conver­sa­tions they engage in. They will do this in partic­u­lar during the exhib­i­tion, always on the lookout, during inter­views with design­ers from French-speak­ing Switzer­land that are part of the programme; they will conduct them in tandem with the mudac curat­ors.",
      "Why do I care?": "",
      "Cf.": "",
      "Type of dataset (input)": "Audio and texts",
      "Result (output)": "Interviews",
      "Formula": "Live audio dataset with documents and information from the WEB",
      "Tools": "",
      "Context": "Mudac Lausanne",
      "Additional info": "Under the direc­tion of Scott Long­fel­low, the artists of studio oio immersed them­selves in the work­ings and vision of mudac and the wider land­scape of French-speak­ing Switzer­land by inter­view­ing over 20 local design­ers, arch­iv­ists, and tech­no­logy experts. This year-long resid­ency gave rise to the Archivives project, designed as an archiv­ing system that collects docu­ments and inter­views to explore, using arti­fi­cial intel­li­gence, the unpre­ced­en­ted possib­il­it­ies of what archiv­ing in the future could look like. New tools for enrich­ing and diver­si­fy­ing archived data have been developed to take account of the context in which design projects are developed and how they are perceived, partic­u­larly by differ­ent audi­ences.\n\nThe project first takes the form of inter­act­ive inter­views with design­ers in the exhib­i­tion hall, conduc­ted by both a member of the museum’s curat­orial team and AI. This process explores how the digital archives selec­ted and trans­mit­ted by the design­ers can be enriched through inter­ac­tion with AI.\nAt the same time, studio oio spec­u­lates on the possib­il­it­ies offered by AI and, at the end of the inter­views, offers a digital poster for a mono­graphic exhib­i­tion that imagines what an exhib­i­tion on design might look like in 10 to 15 years’ time.\n\nThe Archivives project is made up of two tools: Le Curieux and Le Rêveur, the fruit of oio’s resid­ency at mudac and developed as part of the Archives du Design Roman exhib­i­tion.",
      "Links": "https://mudac.ch/en/article/studio-oio-and-artificial-intelligence/",
      "Image": "",
      "Source image": "The inquisitive mind, oio studio, 2024",
      "Addtional links": ""
    },
    {
      "Title": "The Dreamer ",
      "Date": "2024",
      "Authors": "oio Studio",
      "Concept": "Collection",
      "Category": "",
      "Synopsis": "The dreamer is a stor­age device that encodes the inform­a­tion and know­ledge gathered; it creates a separ­ate scal­able arti­fi­cial intel­li­gence model for each designer. These AI models are not only data­bases that store dynamic inform­a­tion, but they also have the abil­ity to gener­ate unique images and texts.",
      "Why do I care?": "",
      "Cf.": "",
      "Type of dataset (input)": "Dynamic information",
      "Result (output)": "Unique texts and images",
      "Formula": "Separate scalable artificial intelligence model",
      "Tools": "",
      "Context": "Mudac Lausanne",
      "Additional info": "Under the direc­tion of Scott Long­fel­low, the artists of studio oio immersed them­selves in the work­ings and vision of mudac and the wider land­scape of French-speak­ing Switzer­land by inter­view­ing over 20 local design­ers, arch­iv­ists, and tech­no­logy experts. This year-long resid­ency gave rise to the Archivives project, designed as an archiv­ing system that collects docu­ments and inter­views to explore, using arti­fi­cial intel­li­gence, the unpre­ced­en­ted possib­il­it­ies of what archiv­ing in the future could look like. New tools for enrich­ing and diver­si­fy­ing archived data have been developed to take account of the context in which design projects are developed and how they are perceived, partic­u­larly by differ­ent audi­ences.\n\nThe project first takes the form of inter­act­ive inter­views with design­ers in the exhib­i­tion hall, conduc­ted by both a member of the museum’s curat­orial team and AI. This process explores how the digital archives selec­ted and trans­mit­ted by the design­ers can be enriched through inter­ac­tion with AI.\nAt the same time, studio oio spec­u­lates on the possib­il­it­ies offered by AI and, at the end of the inter­views, offers a digital poster for a mono­graphic exhib­i­tion that imagines what an exhib­i­tion on design might look like in 10 to 15 years’ time.\n\nThe Archivives project is made up of two tools: Le Curieux and Le Rêveur, the fruit of oio’s resid­ency at mudac and developed as part of the Archives du Design Roman exhib­i­tion.",
      "Links": "https://mudac.ch/en/article/studio-oio-and-artificial-intelligence/",
      "Image": "",
      "Source image": "The dreamer, oio studio, 2024",
      "Addtional links": ""
    },
    {
      "Title": "Infinite Loop ",
      "Date": "2022",
      "Authors": "Silvia Weidenbach \nJon Emmony",
      "Concept": "Collection",
      "Category": "",
      "Synopsis": "Our project, Infinite Loop, explores these topics — feeding an Artificial Intelligence with historical rings, spanning over 3000 years of human creation (including pieces from the Antikensammlungen Munich, the Germanisches Nationalmuseum Nuremberg and The Alice and Louis Koch Collection in The Swiss National Museum, Zurich); each embedded with the desires, dreams and realities of the makers of the time. The AI learns from these pieces of jewellery — examining in its own way the meaning of materiality, form and features. Once it has learnt, it begins its own journey of creation — showing us infinite new rings that feel familiar yet somehow distant.",
      "Why do I care?": "",
      "Cf.": "",
      "Type of dataset (input)": "Collection of images of historical rings from various colletion",
      "Result (output)": "Images",
      "Formula": "",
      "Tools": "",
      "Context": "",
      "Additional info": "",
      "Links": "https://www.silviaweidenbach.com/infiniteloop",
      "Image": "",
      "Source image": "Infinite Loop, Silvia Weidenbach and Jon Emmony, 2022",
      "Addtional links": ""
    },
    {
      "Title": "Wind verification ",
      "Date": "2021",
      "Authors": "Guo Cheng",
      "Concept": "Classifier",
      "Category": "",
      "Synopsis": "\"Wind Verification\" is a research-creation project based on the virtual anthropological field of social media networks in the context of mass surveillance. The installation attempts to reproduce the observable but invisible object - wind - in short videos uploaded by social network users in an indoor space. ",
      "Why do I care?": "",
      "Cf.": "",
      "Type of dataset (input)": "Selected short videos with flags, and 3D images to complete the dataset ",
      "Result (output)": "Movement ",
      "Formula": "Computer vision ",
      "Tools": "",
      "Context": "CAC DKU",
      "Additional info": "Based on the anthropological field of social media networks in the context of mass surveillance, \"Wind Verification\" uses a typological approach based on computer vision technology to analyze video clips with flags on social media. The installation generates the wind that changes with the switching of video clips, and presents it in a visual and sensorial way. While the project showing the possibilities of bottom-up data collection, analysis and processing, it is also presenting a metaphor for the current era: the digital reality is becoming new reality.",
      "Links": "https://www.guo-cheng.net/index.php/projects/wind-verification/",
      "Image": "",
      "Source image": "Wind Verification, Guo Cheng, 2021",
      "Addtional links": ""
    },
    {
      "Title": "The Call ",
      "Date": "2024",
      "Authors": "Holly Herndon \nMat Dryhurst ",
      "Concept": "Training",
      "Category": "",
      "Synopsis": "Throughout the exhibition, a series of artefacts and instruments act as physical representations of the ai\ntraining process. These objects are adorned with narrative\nimagery that communicates the beliefs and values\nembedded in these processes. They suggest that these\nai training protocols could evolve into rituals for making\nart with ai, much like liturgical objects found in places\nof worship",
      "Why do I care?": "",
      "Cf.": "",
      "Type of dataset (input)": "Selected Sacred Harp songs, spatialised field recordings of fifteen local choirs throughout the UK and other data from their composition ",
      "Result (output)": "Songs, Voices, Chorus",
      "Formula": "a songbook of hymnals, singing exercises and a recording protocol. The Songbook was specially developed\nby Herndon and Dryhurst to make training data\nfor choral ai models. The music for the hymns\nwas written using an ai model, Hymnai, trained\non selected Sacred Harp repertoire",
      "Tools": "",
      "Context": "Exhibition at the Serpentine, London UK",
      "Additional info": "“If all media is training data, including art, let’s turn the production of training data into art instead.” – Holly Herndon & Mat Dryhurst\nThe models used to make these compositions are currently considered cutting-edge in a rapidly developing field. In time, the sonic qualities heard in this work will age and become uniquely\nidentifiable as coming from 2024.",
      "Links": "https://d37zoqglehb9o7.cloudfront.net/uploads/2024/08/EXHIBITION-GUIDE-DIGITAL-DEC.pdf \n",
      "Image": "",
      "Source image": "© Leon Chew, The Call, Holly Herndon and Mat Dryhurst with sub, Serpentine, 2024",
      "Addtional links": "All Media is Training Data\nHolly Herndon and Mat Dryhurst\nWritten and Edited by Eva Jäger and Caroline Busta, 2024\nhttps://shop.serpentinegalleries.org/products/holly-herndon-mat-dryhurst-all-media-is-training-data"
    },
    {
      "Title": "Objective Portrait",
      "Date": "2022",
      "Authors": "Vera van der Burg\nGijs de Boer",
      "Concept": "Training",
      "Category": "",
      "Synopsis": "This project explores whether a differently trained object recognition algorithm can help to understand such relations. If we explicitly train an algorithm on someone’s personal values, can it capture something of their particular way of seeing? While an objec recognition algorithm usually tries to learn the definition of an object, we explore the potential for a subject recognition algorithm: can an AI learn to see the value and meaning of an object for someone?\n",
      "Why do I care?": "",
      "Cf.": "",
      "Type of dataset (input)": "A collection of images annoted with subjective labels according to the personal values of each curator. Number of images ranged from 378 to 120. \n\n",
      "Result (output)": "Installation ",
      "Formula": "AI as a reflective design partner. A play with biais, a connection between the maker of the training and its resulat. Three designers annotate a collection of images\nrepresenting their fascinations, with subjective labels, indicating different dimensions of their visual concepts. These labels are used to teach an object detection model\nthe designers’ perspectives. Then, they used this trained model on their own design work to evaluate the AI’s potential to prompt self-reflection. ",
      "Tools": "Model used YOLOv5 \nRoboflow to annotate each image",
      "Context": "Dutch Design week ",
      "Additional info": "“Collecting a dataset is a tedious task, because you need a lot images.\nHowever, collecting a bunch of images that fascinate you feels actually\nless tedious, because I really enjoy looking at them. It feels a bit like\ncreating an archive of your interests, which I as a designer tend to do\nanyways in my practice, for inspiration” - Vera van der Burg ",
      "Links": "https://veravanderburg.nl/",
      "Image": "",
      "Source image": "Objective Portrait, Installation, Dutch Design Week, 2022\nVera van der Burg and Gijs de Boer ",
      "Addtional links": "an der Burg, Vera, Gijs de Boer, Almila Akdag Salah, Senthil Chandrasegaran, and Peter Lloyd. \"Title of the Article.\" Journal Name Volume(Issue) (2024) \nhttps://dl.acm.org/doi/pdf/10.1145/3563657.3595974\n"
    },
    {
      "Title": "Archive of lost mothers ",
      "Date": "2023",
      "Authors": "Computational Mama",
      "Concept": "Worldbuilding",
      "Category": "",
      "Synopsis": "This project has been created using generative AI tools. The work speculates about the existence of post-human mothers and birthing through AI generated images. The images seemingly appear archival yet clearly seem fictional at their edges. Can we generate our own archives with AI? Can we train AI to build scenarios of lost/un-occurred histories with our own images and objects.",
      "Why do I care?": "",
      "Cf.": "",
      "Type of dataset (input)": "Prompt, collection of images ",
      "Result (output)": "Fictional archive",
      "Formula": "The artist used AI image generation tools like Midjourney and Stable Diffusion to create surreal, futuristic images of \"post-human moms\" with evolved features like multiple arms or tentacles. She then crafted narratives around these images, initially writing the stories herself before training an AI to generate them.",
      "Tools": "Midjourney, Stable Diffusion, Huggingface ",
      "Context": "",
      "Additional info": "The artist aimed to explore the \"silences\" and absences in AI representations of motherhood, particularly from non-Western perspectives. By creating surreal, evolved mothers, she sought to speculate on how motherhood might adapt to impossible demands. The project also serves as a commentary on AI biases and the potential loss of diverse representations as AI image generation becomes more prevalent. The interactive nature of the archive invites viewers to engage with these ideas and contribute their own perspectives.",
      "Links": "https://archiveoflostmothers.in/index.html",
      "Image": "",
      "Source image": "AI generated images from the archive of lost mothers, computational mama, 2023",
      "Addtional links": "https://computationalmama.xyz/projects/lostmothers/\nhttps://huggingface.co/computational-mama?sort_models=alphabetical#models\nhttps://gooey.ai/chat/measuring-silences-B49/?fbclid=PAZXh0bgNhZW0CMTEAAaa3XLc6APFckv1liL_DXRKFpnydT3GSC_uYxxMbbhCJZ8_QIlLd8Dd7iZ0_aem_j-5yh4LittbQPrrZZPhfvA"
    },
    {
      "Title": "Psychedelic Forms",
      "Date": "2022",
      "Authors": "Varvara & Mar ",
      "Concept": "Chance",
      "Category": "",
      "Synopsis": "Psychedelic Forms is a series of sculptures that explore deep learning (DL) possibilities for creating a form guided by text prompt and 3D model. The selected generated digital 3D objects were then manually altered and prepared for 3D printing in ceramics. Glazing happened by hand, often inspired by the digital model’s AI-generated vertex colors.",
      "Why do I care?": "",
      "Cf.": "",
      "Type of dataset (input)": "",
      "Result (output)": "3D printed ceramic sculptures glazed by hand",
      "Formula": "Text prompt and 3D model qnd chance into the DL quotation\n",
      "Tools": "",
      "Context": "Espacio Open residency program and Cultural Endowment of Estonia.",
      "Additional info": "Psychedelic refers to unexpected or unexplored imagination that the human eye has not seen before. Although the original input was well-known antient sculptures, like Venus, the AI model was capable of sytilising the mesh with the inputted text prompt in such a way that the new form was hardly recognisible. \n\nAs we know, DL is known for the vast amount of content generation with its own style and aesthetic, which feels repetitive in the long run. However, it is impossible to achieve exact repetition from the same input. In the case of this project, the repetition was further destroyed or made impossible by adding material, physical forces, and chance into the quotation.\n\nAs a result, the neuro-avant-garde mixed with artisan techniques and processes offered irregular transformations that contribute to creativity and imagination augmentation. In other words, irregular mutations can lead to new creations that would not happen otherwise.",
      "Links": "https://var-mar.info/psychedelic-forms/",
      "Image": "",
      "Source image": "Psychedelic forms, Varara & Mar, 2022",
      "Addtional links": "https://www.flickr.com/photos/mcanet/albums/72177720299890759/"
    },
    {
      "Title": "Asunder",
      "Date": "2020",
      "Authors": "Tega Brain, Julian Oliver, and Bengt Sjölén.",
      "Concept": "Simulation",
      "Category": "",
      "Synopsis": "Asunder responds to a growing interest in the application of AI to critical environmental challenges by situating this approach as a literal proposition, combining state of the art climate and environmental simulation technology, a 144 CPU super-computer and Machine Learning image-making techniques. The result is a fictional 'environmental manager' that proposes and simulates future alterations to the planet to keep it safely within planetary boundaries, with what are often completely unacceptable or absurd results. In doing so, Asunder questions assumptions of computational neutrality, our increasingly desperate reach for techno-solutionist fixes to planetary challenges, and the broader ideological framing of the environment as a system.",
      "Why do I care?": "",
      "Cf.": "",
      "Type of dataset (input)": "Sattelite maps, climate modelling data",
      "Result (output)": "3D printed ceramic sculptures glazed by hand",
      "Formula": "Custom-trained climate model",
      "Tools": "",
      "Context": "Asunder was commissioned by the MAK for the VIENNA BIENNALE 2019. Asunder was commissioned by the MAK for the VIENNA BIENNALE 2019. ",
      "Additional info": "Satellite, climate, geology, biodiversity and topography for a series of terrestrial regions is used to generate an ever evolving series of environmental management plans where human and non-human agendas are combined and weighted in different ways. A central argument made by those encouraging the uptake of AI, is that data driven systems can depoliticize or neutralize decision making. Extended to the context of ecosystems, this could imply that ecological agendas are prioritized over human goals (and over the status quo where human systems of production are preserved at the expense of everything else).\n\nThe work is structured into discrete simulations for different regions, positioning ecosystem as computational surface. As cities are relocated, nations combined, coastlines straightened or rivers moved, the work shifts between humorous to proposterous, from uncannily eco-fetishistic to tediously bureaucratic.",
      "Links": "https://asunder.earth/",
      "Image": "",
      "Source image": "Exhibition views from the The Eternal Network, Transmediale 2020. Photography by Luca Girardini, CC NC-SA 4.0.",
      "Addtional links": "https://tegabrain.com/Asunder"
    },
    {
      "Title": "Abstract Language Model (Live)",
      "Date": "2022",
      "Authors": "Andreas Lutz",
      "Concept": "Training",
      "Category": "",
      "Synopsis": "Abstract Language Model consists of complex data models containing the translation of all available human sign systems as equally representable, machine-created states. Through extraction and interpolation of these artificially created semiotic systems a transitionless universal language originates, which can be seen as a trans-human / trans-machine language.",
      "Why do I care?": "",
      "Cf.": "",
      "Type of dataset (input)": "An artificial neural network was trained with the entire charactersets represented in the Unicode Standard",
      "Result (output)": "Live AV performance",
      "Formula": "Process documentation: https://vimeo.com/803965982?turnstile=0.bAMpIGsaxX0aeu7GBsfKsO7msbCwGSlQySvBe6UsrrAc3z_L1i3e-ymEM-KbrbNvGHmqRRMm_4zygETK7Qn6RPqUeuGv_Nm7CObD9e6siMJsjYqGhh2xTJXRa9gHwMDnGvg1c6iT1AWYgXSUQ61S76m2nveU_rMPSUa1BL7Z2c9gAjD37f6dGRIX1kwpT2Z-Z7OVFUbmbHOqexRDbUwg6D85JFaqtodX-NCK5UECeCBNbRSY_XDn25Z1C4TJGqzvpoGAftk5qjgPF2QTCYN8sGG-pTkaE2QVL2ibxmSp83mka1LBY58YBiu3AfW5wNNh2nyJSHfBHtLGgrbWfOXzvz4HV6SMX_Pd3XbV-JQP48Yf75aqq9RvBwM4nIWiH4CJTKSDQ5R1f4-Hng9uKNTWgU4IULtSSn5OWXctS0GGhJaLB5XhG-sktHNXbOmq8DVTVhxG4Beo96aAboNV_U7jNWk1lxtCxwe0b_7rd5IAiqxE809l62OS4ryxBrNKpfbgQTS16lXSpnNMqZeaNPR-iZJPNqUKfdhRcv5jLmUYyPmSYd11nN9kVhy0uLxBNq_vWVxpWBHrS4f5Z5fEj8Xe9ftcXMqDWyxQG_jNmowWNud3iBhzgDcI_C31jGcJAVNjJmR20sXGZOZKXKs0KnDrofZW1IF6OMkYu0J1mHqptSGVvIh4rKtShf0RzJZbJqoIN7nSj7FKXYIPl65Kxx4dd8e-SX3utMKfz8JMA9EMZzKks4R2z5oiBAponcm6YFXlyyx9RWDVL2nMqO5RooxqsHNH_4gIwsY-d7eoEIBb_V1alok7ufiX1_Q-XI2KmcQOS3GxN19kcpopf6KhpDmsgA.HQALSBDVJ2VSAvMvuUlGfw.b8758d9bb5f81874f57bcabc11fb305456c72506526e367e5289b8b8901be644",
      "Tools": "",
      "Context": "",
      "Additional info": "https://andreaslutz.com/monolith-yw-alm/",
      "Links": "https://andreaslutz.com/abstract-language-model/",
      "Image": "",
      "Source image": "In situ at WeSA AudioVisual Festival in Jeju, Jeju / South Korea, 2024 (Photos by Danny Arens)",
      "Addtional links": ""
    },
    {
      "Title": "Ultrachunk",
      "Date": "2018",
      "Authors": "Memo Akten, Jennifer Walsche",
      "Concept": "Doppelgänger",
      "Category": "",
      "Synopsis": "ULTRACHUNK is a collaboration between performer Jennifer Walshe, and artist Memo Akten. At once surreal, spellbinding and deeply alarming, ULTRACHUNK is a live improvisational duet between a classically-trained musician and her AI doppelganger.",
      "Why do I care?": "",
      "Cf.": "",
      "Type of dataset (input)": "Voice and video (webcam)",
      "Result (output)": "Live performance",
      "Formula": "Over the course of a year, Irish vocalist and composer Walshe engaged in a daily ritual of performing solo improvisations in front of her webcam. Collecting the hours of footage on the other end, computational artist Akten used the video and audio material to create and train a machine learning driven system that can mimic the key components of Walshe’s identity – her voice and face.",
      "Tools": "",
      "Context": "Created as part of Memo Akten's PhD at Goldsmiths, University of London, funded by the EPSRC UK.",
      "Additional info": "",
      "Links": "https://www.rewirefestival.nl/artist/jennifer-walshe-memo-akten, https://www.memo.tv/works/ultrachunk/",
      "Image": "",
      "Source image": "Excerpts from performance at ASSEMBLY 2018, Somerset House Studios",
      "Addtional links": ""
    }
   ]